---
title: "Lending Club Data Cleaning"
author: "Evan Woods"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.width = 6)
knitr::opts_chunk$set(fig.asp = 0.618)
knitr::opts_chunk$set(out.width = "70%")
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(
  comment = ""
)
```

```{r message=FALSE, include=FALSE}
if(!require("MASS")) install.packages("MASS")
if(!require("ISLR2")) install.packages("ISLR2")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("HH")) install.packages("HH") # VIF
if(!require("e1071")) install.packages("e1071") # naiveBayes
if(!require("class")) install.packages("class") # knn
if(!require("formulaic")) install.packages("formulaic")
if(!require("caTools")) install.packages("caTools")
if(!require("caret")) install.packages("caret")
if(!require("boot")) install.packages("boot")
if(!require("leaps")) install.packages("leaps") # regsubsets
if(!require("glmnet")) install.packages("glmnet") # Ridge and Lasso Regression
if(!require("pls")) install.packages("pls") # Partial Least Squares & Principal Component Regression
if(!require("splines")) install.packages("splines")
if(!require("gam")) install.packages("gam")
if(!require("akima")) install.packages("akima")
if(!require("tree")) install.packages("tree") # Classification and Regression Trees
if(!require("randomForest")) install.packages("randomForest")
if(!require("gbm")) install.packages("gbm") # Boosted Trees
if(!require("BART")) install.packages("BART")
if(!require("reticulate")) install.packages("reticulate") # Use python objects in R
if(!require("ROCR")) install.packages("ROCR")
if(!require("keras")) install.packages("keras") # Install keras for deep learning
if(!require("magritter")) install.packages("magritter") 
if(!require("lubridate")) install.packages("lubridate")
if(!require("sjmisc")) install.packages("sjmisc") # overloading is_empty
if(!require("randomForest")) install.packages("randomForest")
if(!require("tree")) install.packages("tree")

library(randomForest)
library(tree)
library(sjmisc) # overloading is_empty
library(lubridate)
library(magrittr)
library(keras)
reticulate::use_condaenv(condaenv = "r-tensorflow")
library(ROCR)
library(reticulate)
library(BART)
library(gbm)
library(randomForest)
library(tree)
library(akima)
library(gam)
library(splines)
library(glmnet)
library(pls)
library(leaps)
library(formulaic)
library(class)
library(e1071)
library(HH)
library(MASS)
library(ISLR2)
library(tidyverse)
library(caTools)
library(caret)
library(boot)
```

```{r include=FALSE}
custom_darkblue = "#1A0875"
custom_lightblue = "#34ABEB"
custom_red = "#a60808"
```

## Function Definitions
```{r include=FALSE}
f_print <- function(string){
  cat(str_wrap(string = string, width = 80, indent = 0, exdent = 0, whitespace_only = TRUE))
}
```

```{r}
# Identifying All Character Values
# """
# Purpose: This function will identify all character types within a set of data
# Parameters: 
#             data: This is expected to be a dataframe of training or test data.
# Returns: This function returns a collection of names of the predictors that 
#          are of type character.
# """
identify_character_values <- function(data){
  data_chr <- character()
  for (i in seq_along(names(data))){
    names_data <- names(data)
    if(is.character(data[[i]])){
      data_chr[[i]] <- names_data[i]
    }
  }  
  return(data_chr)
}
```

```{r}
# """
# Purpose: This function will clean character data. The data that needs to be 
#          formatted as numeric will be cleaned of extraneous characters before
#          converting the data to a numeric data type. It will check columns for
#          non-unique values and drop those columns from the dataframe. It 
#          verifies that all values of the next_payment_d column are empty, and
#          if so, the column is dropped from the dataframe. The proportion of 
#          missing data is then calculated and character columns are converted
#          to factors. The url & description columns are left as of type 
#          character. This function is meant to be called before identifying 
#          the proportion of missing numeric values. 
# 
# Parameters: 
#             data: This is expected to be a dataframe of training or test data.
#             verbose: This will allow print statements to be suppressed when 
#                      FALSE. Default is set to TRUE.
# Returns: This function returns a list containing the cleaned dataframe, and
#          a collection of the character columns that were dropped.
# """
clean_char_data <- function(data, verbose = TRUE){
  # Identify character columns
  data_character_columns <- character()
  data_character_columns_index <- 1
  for (i in seq_along(names(data))){
    names_data <- names(data)
    if(is.character(data[[i]])){
      data_character_columns[[data_character_columns_index]] <- names_data[i]
      data_character_columns_index <- data_character_columns_index + 1
    }
  }
  
  # Transform character data to numeric in months
  data[data_character_columns]['int_rate'] <- str_replace(data[data_character_columns]$int_rate, "\\%", "")
  data[data_character_columns]['int_rate'] <- str_replace(data[data_character_columns]$int_rate, "[ ]", "")
  data[data_character_columns]['int_rate'] <- as.numeric(data[data_character_columns]$int_rate)  

  # data[data_character_columns]['term'] <- str_replace(data[data_character_columns]$term, "[ ]+(months)", "")
  # data[data_character_columns]['term'] <- str_replace(data[data_character_columns]$term, "[ ]", "")
  data[data_character_columns]['term'] <- as.factor(data[data_character_columns]$term)

  data[data_character_columns]['revol_util'] <- str_replace(data[data_character_columns]$revol_util, "\\%", "")
  data[data_character_columns]['revol_util'] <- as.numeric(data[data_character_columns]$revol_util)
  
  # Checking for columns with non-unique values
  data_character_columns_to_drop <- character()
  column_count_to_drop = 1
  for (i in seq_along(data_character_columns)){
    current_col <- data_character_columns[i]  
      
    if(length(unique(as.factor(data[,data_character_columns[i]]))) == 1){
      data_character_columns_to_drop[column_count_to_drop] <- data_character_columns[i]
      column_count_to_drop = column_count_to_drop + 1
    }
  }
  
  if(verbose){
    print("Dropping columns due to non-unique values:")
    print(data_character_columns_to_drop)
  }
  
  # Checking for non-empty strings
  found_non_empty_string <- FALSE
  for (i in length(data[data_character_columns]$next_pymnt_d)){
    if (!(is_empty(data[data_character_columns]$next_pymnt_d[[i]]))){
      found_non_empty_string <- TRUE
      print(data[data_character_columns]$next_pymnt_d[[i]])
      break
    }
  }
  if(!found_non_empty_string) {
    data_character_columns_to_drop[column_count_to_drop] <- 'next_pymnt_d'
    column_count_to_drop <- column_count_to_drop + 1
  }
  
  # Drop columns that have only singly unique values
  data %<>% select(everything(), -data_character_columns_to_drop)
  
  # Mutate character data to factors
  data %<>% mutate_if(is.character, as.factor)
  
  # Retain url & desc as character data
  data <- data %>% mutate(url = as.character(url)) %>% mutate(desc = as.character(desc))
  
  result <- list(data, data_character_columns_to_drop)
  return(result) 
}
```

```{r}
# identify the proportion of missing values
# """
# Purpose: This function will identify the proportion of missing values in a
#          dataframe. It will drop values that have more than 10% missing 
#          values, and otherwise impute numeric values with the mean. The 
#          cleaned data is then included as the first index in the returned
#          list.
# Parameters: 
#             data: This is expected to be a dataframe of training or test data.
#             verbose: This will print the column names and their proportions of
#                      missing values.
# Returns: This function returns a list of proportions of missing data for
#          each predictor in the input dataframe, along with lists of suggested
#          columns to drop given all missing data, columns to drop given more 
#          than 10% missing data, & columns to impute given less than or equal 
#          to 10% missing data. The cleaned dataframe is returned as the first
#          item in the resulting list.
# """
identify_proportion_NA <- function(data, verbose=TRUE){
  names_data <- names(data)
  proportion_missing <- c(integer())
  for (i in seq_along(data)){
      if (is.numeric(data[,names_data[i]]) | is.logical(data[,names_data[i]])){
        proportion_missing[i] <- ((sum(is.na(data[,names_data[i]])) / (length(data[,names_data[i]]))) * 100)
      } else if(is.character(data[,names_data[i]]) | is.factor(data[,names_data[i]])) {
        proportion_missing[i] <- ((length(data[data[names_data[i]] == "", names_data[i]]) / length(data[,names_data[i]])) * 100)
      } else {
        if(verbose){
          print("Warning: unhandled data type when calculating missing proportion.")
          cat("\n")
          f_print(sprintf("index: %0.0f", i))
          cat("\n")
          f_print(sprintf("Column: %s", names_data[i]))
          cat("\n")  
        }
      }
  }
  if(verbose==TRUE){
    print("Columns to drop:")
    cat("\n")  
  }
  columns_to_drop_100_missing <- character()
  for (i in seq_along(data)){
    if(proportion_missing[i] == 100){
      if(names_data[i] != "desc"){
        columns_to_drop_100_missing[i] <- names_data[i]
      }
      if(verbose == TRUE) {
        
        print(names_data[i])
        print(proportion_missing[i])
        cat("\n")  
      }
    }
  }

  if(verbose == TRUE){
    print("Columns to Impute:")
    cat("\n")  
  }  
  columns_to_impute <- character()
  for (i in seq_along(data)){
    if(proportion_missing[i] <= 10 & proportion_missing[i] > 0){
      if(names_data[i] != "desc"){
        columns_to_impute[i] <- names_data[i]
      }
      if(verbose == TRUE){
        print(names_data[i])
        print(proportion_missing[i])
        cat("\n")  
      }
    }
  }

  if(verbose == TRUE){
    print("Columns less than 100% to drop:")
    cat("\n")  
  } 
  
  columns_to_drop_less_than_100 <- character()
  for (i in seq_along(data)){
    if(proportion_missing[i] > 10 & proportion_missing[i] < 100 ){
      if(names_data[i] != "desc"){
        columns_to_drop_less_than_100[i] <- names_data[i]
      }
      if(verbose == TRUE){
        print(names_data[i])
        print(proportion_missing[i])
        cat("\n")  
      }
    }
  }
  
  columns_to_drop_100_missing <- columns_to_drop_100_missing[!is.na(columns_to_drop_100_missing)]
  columns_to_impute <- columns_to_impute[!is.na(columns_to_impute)]
  columns_to_drop_less_than_100 <-columns_to_drop_less_than_100[!is.na(columns_to_drop_less_than_100)]
  
  # Removing columns with all missing values.
  if(verbose == TRUE){
    cat("\n")
    print("Dropping columns with all missing values:")
    cat("\n")
    print(columns_to_drop_100_missing)
  }
  data %<>% select(everything(),-columns_to_drop_100_missing)

  # Removing columns with values with greater than 10% missing values.
  if(verbose == TRUE){
    cat("\n")
    print("Dropping columns with greater than 10% missing values:")
    cat("\n")
    print(columns_to_drop_less_than_100)
  }

  data %<>% select(everything(),-columns_to_drop_less_than_100)
  
  # Imputing Missing Data
  if(verbose){
    cat("\n")
    print("Imputing Columns:")
    cat("\n")
    print(columns_to_impute)
  }
  for (i in seq_along(columns_to_impute))
    if(is.numeric(data[,columns_to_impute[i]])){
      data[,columns_to_impute[i]]
      imputed_mean <- mean(data[,columns_to_impute[i]], na.rm = TRUE)
      data[, columns_to_impute[i]][is.na(data[,columns_to_impute[i]])] <- imputed_mean
    } else {
      sprintf("Warning: unhandled column to impute is non-numeric: %s",columns_to_impute[i])
    }

  results <- list(data, columns_to_drop_100_missing, columns_to_drop_less_than_100, columns_to_impute, proportion_missing, names_data)
  return(results)
}

# id_prop_NA_list <- identify_proportion_NA(train_data)
```

```{r}
# Describe the proportion of a single missing value
# """
# Purpose: This function will identify the proportion of missing values given 
#          the cleaned dataframe from the function "identify_proportion_NA" and
#          the string containing the the variable in question. This function is
#          dependent upon the function "f_print" for custom printing.
#
# Parameters: 
#             proportion_missing_df: This is expected to be a dataframe returned
#                                    from the function "identify_proportion_NA".
#             variable_to_id: This is a string within the dataframe which is of
#                             interest.
# Returns: This function prints the requested variable and the proprotion 
#          missing for that variable.
# """
describe_proportion_missing <- function(proportion_missing_df, variable_to_id){
for (i in seq_along(proportion_missing_df[[6]])){
  if(proportion_missing_df[[6]][i] == variable_to_id)
    f_print(sprintf("%s: %0.3f", proportion_missing_df[[6]][i], proportion_missing_df[[5]][i]))
  }  
}
```

```{r}
train_data <- read.csv('./train_loans2013_2015q1_minimalprocessing.csv')
test_data <- read.csv('./test_loans2013_2015q1_minimalprocessing.csv')
```

## Cleaning Training Data
### Cleaning Character data
```{r}
training_clean_result <- clean_char_data(train_data)
train_data <- training_clean_result[[1]]
train_data
```

```{r}
# Identify columns with missing values in the training data
training_id_prop_NA_list <- identify_proportion_NA(train_data, verbose=FALSE)
```

```{r}
# Identify the factors in the training data
factors <- train_data %>% select_if(is.factor)
names_factors <- names(factors)
```

```{r}
# Identify the proportion of missing values amongst the factors
for (i in seq_along(names_factors)){
  describe_proportion_missing(training_id_prop_NA_list, names_factors[i])
  cat("\n")
}
```

```{r}
# Potential categories for sentiment analysis
# emp_title
# title
# url
# desc

# Potential categories for imputation using knn or kmeans
# emp_length
# last_credit_pull_d
# last_pymnt_d

# Only emp_length is in the test data
```

```{r}
# Update cleaned data
train_data <- training_id_prop_NA_list[[1]]
```

```{r}
# Dropping emp_title, title, url, & description due to irrelevant content
train_data %<>% select(everything(), -emp_title, -title, -url, -desc)
```

### Impute missing values using clustering techniques
```{r}
# emp_length
# last_credit_pull_d
# last_pymnt_d
```

```{r}
# Dropping emp_length, last_credit_pull_d, & last_pymnt_d because not in test data
train_data %<>% select(everything(), -emp_length, -last_credit_pull_d, -last_pymnt_d)
```

```{r}
# Inverting the good_loan logic to be intuitive
train_data$good_loan <- (as.numeric(train_data$loan_status) - 1) 
```

```{r}
# Dropping redundant loan_status col
train_data %<>% select(everything(), -loan_status)
```

```{r}
train_data
```


## Cleaning the Test Data
### Cleaning Character Data

```{r}
test_clean_result <- clean_char_data(test_data)
```

```{r}
test_data <- test_clean_result[[1]]
```

```{r}
# Identify columns with missing values in the test data
test_id_prop_NA_list <- identify_proportion_NA(test_data, verbose=FALSE)
```

```{r}
# Identify the factors in the test data
factors <- test_data %>% select_if(is.factor)
names_factors <- names(factors)
```

```{r}
# Identify the proportion of missing values amongst the factors
for (i in seq_along(names_factors)){
  describe_proportion_missing(test_id_prop_NA_list, names_factors[i])
  cat("\n")
}
```

```{r}
# Potential to impute values using clustering techniques and data-preprocessing
# emp_title

# Potential to impute values using clustering techniques
# emp_length
```

```{r}
test_data <- test_id_prop_NA_list[[1]]
```

### Impute emp_title using clustering techniques
```{r}
# Imputing emp_length using clustering techniques
# test_data$emp_title
```

```{r}
# Dropping emp_title & emp_length to create classifier; able to compare results after imputation later
test_data %<>% select(everything(), -emp_title, -emp_length, -title, -url, -desc)
```

```{r}
(test_data)
# (train_data)
```

```{r}
# Test values that were dropped for quantity of missing values or irrelevant content (title, url, desc, emp_title are irrelevant)
dropped_test_values <- c(test_id_prop_NA_list[[2]], test_id_prop_NA_list[[3]], "emp_title", "emp_length", "title", "url", "desc")
dropped_training_values <- c(training_id_prop_NA_list[[2]], training_id_prop_NA_list[[3]], "emp_title", "emp_length", "last_credit_pull_d", "last_pymnt_d", "title", "url", "desc")
```

```{r}
length(dropped_training_values)
length(dropped_test_values)
```

```{r}
# Verifying The length of training is equal to test
dropped_test_values_only <- (dropped_test_values[!(dropped_test_values %in% dropped_training_values)]) # values that were dropped only in test

# dropped_test_values[(dropped_test_values %in% dropped_training_values)] # values that were dropped in both training & test
# dropped_training_values[!(dropped_training_values %in% dropped_test_values)] # unique dropped training values
```


```{r}
values_not_in_train_data <- c("pymnt_plan", "disbursement_method")
dropped_test_values_only <- dropped_test_values_only[!(dropped_test_values_only %in% values_not_in_train_data)]

train_data %<>% select(everything(), -dropped_test_values_only)
```

```{r}
length(names(train_data))
length(names(test_data))

# test_data
```

```{r}
# All data is present & transformed into either numeric or factors
test_data
```

```{r}
train_data
```

```{r}
# write.csv(train_data, file = 'clean_train_loans2013_2015q1_minimalprocessing.csv')
```

```{r}
# write.csv(test_data, file = 'clean_test_loans2013_2015q1_minimalprocessing.csv')
```


# Decision Tree Classifier

```{r}
train_data_factors <- train_data %>% select_if(is.factor)
names_train_data_factors <- names(train_data_factors)

test_data_factors <- test_data %>% select_if(is.factor)
names_test_data_factors <- names(test_data_factors)
```

```{r}
# Factor predictors must have at most 32 levels; encoding large factors as numeric
for (i in seq_along(names_train_data_factors)){
  print(names_train_data_factors[i])
  print(length(levels(train_data[,names_train_data_factors[i]])))
  if(length(levels(train_data[,names_train_data_factors[i]])) > 32){
    train_data[,names_train_data_factors[i]] <- as.numeric(train_data[,names_train_data_factors[i]])
  }
}
```


```{r}
# Factor predictors must have at most 32 levels; encoding large factors as numeric
for (i in seq_along(names_test_data_factors)){
  print(names_test_data_factors[i])
  print(length(levels(test_data[,names_test_data_factors[i]])))
  if(length(levels(test_data[,names_test_data_factors[i]])) > 32){
    test_data[,names_test_data_factors[i]] <- as.numeric(test_data[,names_test_data_factors[i]])
  }
}

```

```{r}
# Scaling numerically encoded factors between 0 and 1
train_data_max.sub_grade <- which.max(train_data$sub_grade)
train_data_max.zip_code <- which.max(train_data$zip_code)
train_data_max.addr_state <- which.max(train_data$addr_state)
train_data_max.earliest_cr_line <-  which.max(train_data$earliest_cr_line)

train_data$sub_grade <- train_data$sub_grade / train_data_max.sub_grade
train_data$zip_code <- train_data$zip_code / train_data_max.zip_code
train_data$addr_state <- train_data$addr_state / train_data_max.addr_state
train_data$earliest_cr_line <- train_data$earliest_cr_line / train_data_max.earliest_cr_line

# train_data$sub_grade
# train_data$zip_code
# train_data$addr_state
# train_data$earliest_cr_line
```

```{r}
# Scaling test data 
test_data_max.sub_grade <- which.max(test_data$sub_grade)
test_data_max.zip_code <- which.max(test_data$zip_code)
test_data_max.addr_state <- which.max(test_data$addr_state)
test_data_max.earliest_cr_line <-  which.max(test_data$earliest_cr_line)

test_data$sub_grade <- test_data$sub_grade / test_data_max.sub_grade
test_data$zip_code <- test_data$zip_code / test_data_max.zip_code
test_data$addr_state <- test_data$addr_state / test_data_max.addr_state
test_data$earliest_cr_line <- test_data$earliest_cr_line / test_data_max.earliest_cr_line
```


```{r}
# Treating good_loan as a factor to create a classification tree
train_data$good_loan <- as.factor(train_data$good_loan)
```


```{r}
set.seed(42)
tree.good_loan <- tree(good_loan ~ ., data = train_data)
summary(tree.good_loan)
```


```{r}
plot(tree.good_loan)
text(tree.good_loan, pretty = 0)
```


```{r}
# train_data$sub_grade[1]
0.0402098 * train_data_max.sub_grade
```


```{r}
test_data$sub_grade
```


```{r}
tree.pred <- predict(tree.good_loan, test_data, type = "class")

# table(tree.pred, High.test)

# f_print(sprintf("The test accuracy of predicted carseats sales greater than 8 using the fitted classification tree is: %0.3f%%.", ((79 + 62) / 200) * 100))
```


```{r}
tree.pred[tree.pred == 0] # predictions are 1 instead of 0
```


## Selecting features
```{r}
# Potential Features used to predict grade: 
 # num_tl_120dpd_2m, delinq_2yrs, delinq_amnt, num_accts_ever_120_pd, acc_now_delinq, dti
# 
# selected_model_test <- test_data %>% select(num_accts_ever_120_pd, delinq_2yrs, delinq_amnt, num_accts_ever_120_pd, acc_now_delinq, dti, grade)
# selected_model_train <- train_data %>% select(num_accts_ever_120_pd, delinq_2yrs, delinq_amnt, num_accts_ever_120_pd, acc_now_delinq, dti, grade)
# ```
# 
# ```{r}
# write.csv(selected_model_train, file = 'selected_model_train.csv')
# write.csv(selected_model_test, file = 'selected_model_test.csv')
```

```{r}
# Identifying outliers & High leverage values
```


```{r}
# Fitting a lasso regression to see which predictors may be eliminated.
```


```{r}
# x_train <- train_data %>% select(everything(), -grade)
# y_train <- train_data %>% select(grade)
# 
# x_test <- test_data %>% select(everything(), -grade)
# y_test <- test_data %>% select(grade)
# ```
# 
# ```{r}
# x_train_numeric <- x_train %>% mutate_if(is.factor, as.numeric)
# y_train_numeric <- y_train %>% mutate_if(is.factor, as.numeric)
# x_test_numeric <- x_test %>% mutate_if(is.factor, as.numeric)
# y_test_numeric <- y_test %>% mutate_if(is.factor, as.numeric)
```

```{r}
# x_train_numeric
```


```{r}
x <- model.matrix(grade ~ ., data = train_data)[, -1] # -1 removes the intercept
y <- train_data$grade
```


```{r}
set.seed(42)
grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x_train_numeric, y_train_numeric, alpha = 1, lambda = grid)
plot(lasso.mod)
```

```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)
plot(cv.out)
cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s = cv.out$lambda.min, newx = x[test, ])
mean((lasso.pred - y.test)^2)
```


```{r}
 out <- glmnet(x, y, lambda = grid, alpha = 1)
lasso.coef <- predict(out, s = cv.out$lambda.min, type = "coefficients")[1:20, ]
lasso.coef
```


