---
title: "Lending Club Data Cleaning"
author: "Evan Woods"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
# knitr::opts_chunk$set(fig.width = 6)
# knitr::opts_chunk$set(fig.asp = 0.618)
# knitr::opts_chunk$set(out.width = "70%")
# knitr::opts_chunk$set(fig.align = "center")
# knitr::opts_chunk$set(
#   comment = ""
# )
```

```{r message=FALSE, include=FALSE}
if(!require("MASS")) install.packages("MASS")
if(!require("ISLR2")) install.packages("ISLR2")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("HH")) install.packages("HH") # VIF
if(!require("e1071")) install.packages("e1071") # naiveBayes
if(!require("class")) install.packages("class") # knn
if(!require("formulaic")) install.packages("formulaic")
if(!require("caTools")) install.packages("caTools")
if(!require("caret")) install.packages("caret")
if(!require("boot")) install.packages("boot")
if(!require("leaps")) install.packages("leaps") # regsubsets
if(!require("glmnet")) install.packages("glmnet") # Ridge and Lasso Regression
if(!require("pls")) install.packages("pls") # Partial Least Squares & Principal Component Regression
if(!require("splines")) install.packages("splines")
if(!require("gam")) install.packages("gam")
if(!require("akima")) install.packages("akima")
if(!require("tree")) install.packages("tree") # Classification and Regression Trees
if(!require("randomForest")) install.packages("randomForest")
if(!require("gbm")) install.packages("gbm") # Boosted Trees
if(!require("BART")) install.packages("BART")
if(!require("reticulate")) install.packages("reticulate") # Use python objects in R
if(!require("ROCR")) install.packages("ROCR")
if(!require("keras")) install.packages("keras") # Install keras for deep learning
if(!require("magritter")) install.packages("magritter") 
if(!require("lubridate")) install.packages("lubridate")
if(!require("sjmisc")) install.packages("sjmisc") # overloading is_empty
if(!require("randomForest")) install.packages("randomForest")
if(!require("tree")) install.packages("tree")
if(!require("rfUtilities")) install.packages("rfUtilities")

library(rfUtilities)
library(randomForest)
library(tree)
library(sjmisc) # overloading is_empty
library(lubridate)
library(magrittr)
library(keras)
reticulate::use_condaenv(condaenv = "r-tensorflow")
library(ROCR)
library(reticulate)
library(BART)
library(gbm)
library(randomForest)
library(tree)
library(akima)
library(gam)
library(splines)
library(glmnet)
library(pls)
library(leaps)
library(formulaic)
library(class)
library(e1071)
library(HH)
library(MASS)
library(ISLR2)
library(tidyverse)
library(caTools)
library(caret)
library(boot)
```

```{r include=FALSE}
custom_darkblue = "#1A0875"
custom_lightblue = "#34ABEB"
custom_red = "#a60808"
```

## Function Definitions
```{r include=FALSE}
f_print <- function(string){
  cat(str_wrap(string = string, width = 80, indent = 0, exdent = 0, whitespace_only = TRUE))
}
```

```{r}
# Identifying All Character Values
# """
# Purpose: This function will identify all character types within a set of data
# Parameters: 
#             data: This is expected to be a dataframe of training or test data.
# Returns: This function returns a collection of names of the predictors that 
#          are of type character.
# """
identify_character_values <- function(data){
  data_chr <- character()
  for (i in seq_along(names(data))){
    names_data <- names(data)
    if(is.character(data[[i]])){
      data_chr[[i]] <- names_data[i]
    }
  }  
  return(data_chr)
}
```

```{r}
# """
# Purpose: This function will clean character data. The data that needs to be 
#          formatted as numeric will be cleaned of extraneous characters before
#          converting the data to a numeric data type. It will check columns for
#          non-unique values and drop those columns from the dataframe. It 
#          verifies that all values of the next_payment_d column are empty, and
#          if so, the column is dropped from the dataframe. The proportion of 
#          missing data is then calculated and character columns are converted
#          to factors. The url & description columns are left as of type 
#          character. This function is meant to be called before identifying 
#          the proportion of missing numeric values. 
# 
# Parameters: 
#             data: This is expected to be a dataframe of training or test data.
#             verbose: This will allow print statements to be suppressed when 
#                      FALSE. Default is set to TRUE.
# Returns: This function returns a list containing the cleaned dataframe, and
#          a collection of the character columns that were dropped.
# """
clean_char_data <- function(data, verbose = TRUE){
  # Identify character columns
  data_character_columns <- character()
  data_character_columns_index <- 1
  for (i in seq_along(names(data))){
    names_data <- names(data)
    if(is.character(data[[i]])){
      data_character_columns[[data_character_columns_index]] <- names_data[i]
      data_character_columns_index <- data_character_columns_index + 1
    }
  }
  
  # Transform character data to numeric in months
  data[data_character_columns]['int_rate'] <- str_replace(data[data_character_columns]$int_rate, "\\%", "")
  data[data_character_columns]['int_rate'] <- str_replace(data[data_character_columns]$int_rate, "[ ]", "")
  data[data_character_columns]['int_rate'] <- as.numeric(data[data_character_columns]$int_rate)  

  # data[data_character_columns]['term'] <- str_replace(data[data_character_columns]$term, "[ ]+(months)", "")
  # data[data_character_columns]['term'] <- str_replace(data[data_character_columns]$term, "[ ]", "")
  data[data_character_columns]['term'] <- as.factor(data[data_character_columns]$term)

  data[data_character_columns]['revol_util'] <- str_replace(data[data_character_columns]$revol_util, "\\%", "")
  data[data_character_columns]['revol_util'] <- as.numeric(data[data_character_columns]$revol_util)
  
  # Checking for columns with non-unique values
  data_character_columns_to_drop <- character()
  column_count_to_drop = 1
  for (i in seq_along(data_character_columns)){
    current_col <- data_character_columns[i]  
      
    if(length(unique(as.factor(data[,data_character_columns[i]]))) == 1){
      data_character_columns_to_drop[column_count_to_drop] <- data_character_columns[i]
      column_count_to_drop = column_count_to_drop + 1
    }
  }
  
  if(verbose){
    print("Dropping columns due to non-unique values:")
    print(data_character_columns_to_drop)
  }
  
  # Checking for non-empty strings
  found_non_empty_string <- FALSE
  for (i in length(data[data_character_columns]$next_pymnt_d)){
    if (!(is_empty(data[data_character_columns]$next_pymnt_d[[i]]))){
      found_non_empty_string <- TRUE
      print(data[data_character_columns]$next_pymnt_d[[i]])
      break
    }
  }
  if(!found_non_empty_string) {
    data_character_columns_to_drop[column_count_to_drop] <- 'next_pymnt_d'
    column_count_to_drop <- column_count_to_drop + 1
  }
  
  # Drop columns that have only singly unique values
  data %<>% select(everything(), -data_character_columns_to_drop)
  
  # Mutate character data to factors
  data %<>% mutate_if(is.character, as.factor)
  
  # Retain url & desc as character data
  data <- data %>% mutate(url = as.character(url)) %>% mutate(desc = as.character(desc))
  
  result <- list(data, data_character_columns_to_drop)
  return(result) 
}
```

```{r}
# identify the proportion of missing values
# """
# Purpose: This function will identify the proportion of missing values in a
#          dataframe. It will drop values that have more than 10% missing 
#          values, and otherwise impute numeric values with the mean. The 
#          cleaned data is then included as the first index in the returned
#          list.
# Parameters: 
#             data: This is expected to be a dataframe of training or test data.
#             verbose: This will print the column names and their proportions of
#                      missing values.
# Returns: This function returns a list of proportions of missing data for
#          each predictor in the input dataframe, along with lists of suggested
#          columns to drop given all missing data, columns to drop given more 
#          than 10% missing data, & columns to impute given less than or equal 
#          to 10% missing data. The cleaned dataframe is returned as the first
#          item in the resulting list.
# """
identify_proportion_NA <- function(data, verbose=TRUE){
  names_data <- names(data)
  proportion_missing <- c(integer())
  for (i in seq_along(data)){
      if (is.numeric(data[,names_data[i]]) | is.logical(data[,names_data[i]])){
        proportion_missing[i] <- ((sum(is.na(data[,names_data[i]])) / (length(data[,names_data[i]]))) * 100)
      } else if(is.character(data[,names_data[i]]) | is.factor(data[,names_data[i]])) {
        proportion_missing[i] <- ((length(data[data[names_data[i]] == "", names_data[i]]) / length(data[,names_data[i]])) * 100)
      } else {
        if(verbose){
          print("Warning: unhandled data type when calculating missing proportion.")
          cat("\n")
          f_print(sprintf("index: %0.0f", i))
          cat("\n")
          f_print(sprintf("Column: %s", names_data[i]))
          cat("\n")  
        }
      }
  }
  if(verbose==TRUE){
    print("Columns to drop:")
    cat("\n")  
  }
  columns_to_drop_100_missing <- character()
  for (i in seq_along(data)){
    if(proportion_missing[i] == 100){
      if(names_data[i] != "desc"){
        columns_to_drop_100_missing[i] <- names_data[i]
      }
      if(verbose == TRUE) {
        
        print(names_data[i])
        print(proportion_missing[i])
        cat("\n")  
      }
    }
  }

  if(verbose == TRUE){
    print("Columns to Impute:")
    cat("\n")  
  }  
  columns_to_impute <- character()
  for (i in seq_along(data)){
    if(proportion_missing[i] <= 10 & proportion_missing[i] > 0){
      if(names_data[i] != "desc"){
        columns_to_impute[i] <- names_data[i]
      }
      if(verbose == TRUE){
        print(names_data[i])
        print(proportion_missing[i])
        cat("\n")  
      }
    }
  }

  if(verbose == TRUE){
    print("Columns less than 100% to drop:")
    cat("\n")  
  } 
  
  columns_to_drop_less_than_100 <- character()
  for (i in seq_along(data)){
    if(proportion_missing[i] > 10 & proportion_missing[i] < 100 ){
      if(names_data[i] != "desc"){
        columns_to_drop_less_than_100[i] <- names_data[i]
      }
      if(verbose == TRUE){
        print(names_data[i])
        print(proportion_missing[i])
        cat("\n")  
      }
    }
  }
  
  columns_to_drop_100_missing <- columns_to_drop_100_missing[!is.na(columns_to_drop_100_missing)]
  columns_to_impute <- columns_to_impute[!is.na(columns_to_impute)]
  columns_to_drop_less_than_100 <-columns_to_drop_less_than_100[!is.na(columns_to_drop_less_than_100)]
  
  # Removing columns with all missing values.
  if(verbose == TRUE){
    cat("\n")
    print("Dropping columns with all missing values:")
    cat("\n")
    print(columns_to_drop_100_missing)
  }
  data %<>% select(everything(),-columns_to_drop_100_missing)

  # Removing columns with values with greater than 10% missing values.
  if(verbose == TRUE){
    cat("\n")
    print("Dropping columns with greater than 10% missing values:")
    cat("\n")
    print(columns_to_drop_less_than_100)
  }

  data %<>% select(everything(),-columns_to_drop_less_than_100)
  
  # Imputing Missing Data
  if(verbose){
    cat("\n")
    print("Imputing Columns:")
    cat("\n")
    print(columns_to_impute)
  }
  for (i in seq_along(columns_to_impute))
    if(is.numeric(data[,columns_to_impute[i]])){
      data[,columns_to_impute[i]]
      imputed_mean <- mean(data[,columns_to_impute[i]], na.rm = TRUE)
      data[, columns_to_impute[i]][is.na(data[,columns_to_impute[i]])] <- imputed_mean
    } else {
      sprintf("Warning: unhandled column to impute is non-numeric: %s",columns_to_impute[i])
    }

  results <- list(data, columns_to_drop_100_missing, columns_to_drop_less_than_100, columns_to_impute, proportion_missing, names_data)
  return(results)
}

# id_prop_NA_list <- identify_proportion_NA(train_data)
```

```{r}
# Describe the proportion of a single missing value
# """
# Purpose: This function will identify the proportion of missing values given 
#          the cleaned dataframe from the function "identify_proportion_NA" and
#          the string containing the the variable in question. This function is
#          dependent upon the function "f_print" for custom printing.
#
# Parameters: 
#             proportion_missing_df: This is expected to be a dataframe returned
#                                    from the function "identify_proportion_NA".
#             variable_to_id: This is a string within the dataframe which is of
#                             interest.
# Returns: This function prints the requested variable and the proprotion 
#          missing for that variable.
# """
describe_proportion_missing <- function(proportion_missing_df, variable_to_id){
for (i in seq_along(proportion_missing_df[[6]])){
  if(proportion_missing_df[[6]][i] == variable_to_id)
    f_print(sprintf("%s: %0.3f", proportion_missing_df[[6]][i], proportion_missing_df[[5]][i]))
  }  
}
```

```{r}
train_data <- read.csv('./train_loans2013_2015q1_minimalprocessing.csv')
test_data <- read.csv('./test_loans2013_2015q1_minimalprocessing.csv')
```

```{r}
# Encoding variables with mostly missing data as factors then numeric & capturing encodings 

train_data$hardship_type <- as.numeric(as.factor(train_data$hardship_type)) - 1
train_data.encodings.hardship_type <- levels(as.factor(train_data$hardship_type))

train_data$hardship_reason <- as.numeric(as.factor(train_data$hardship_reason)) - 1
train_data.encodings.hardship_reason <- levels(as.factor(train_data$hardship_reason))

train_data$hardship_status <- as.numeric(as.factor(train_data$hardship_status)) - 1
train_data.encodings.hardship_status <- levels(as.factor(train_data$hardship_status))

train_data$hardship_start_date <- as.numeric(as.factor(train_data$hardship_start_date)) - 1
train_data.encodings.hardship_start_date <- levels(as.factor(train_data$hardship_start_date))

train_data$hardship_end_date <- as.numeric(as.factor(train_data$hardship_end_date)) - 1
train_data.encodings.hardship_end_date <- levels(as.factor(train_data$hardship_end_date))

train_data$payment_plan_start_date <- as.numeric(as.factor(train_data$payment_plan_start_date)) - 1
train_data.encodings.payment_plan_start_date <- levels(as.factor(train_data$payment_plan_start_date))

train_data$hardship_loan_status <- as.numeric(as.factor(train_data$hardship_loan_status)) - 1
train_data.encodings.hardship_loan_status <- levels(as.factor(train_data$hardship_loan_status))

train_data$debt_settlement_flag <- as.numeric(as.factor(train_data$debt_settlement_flag)) - 1
train_data.encodings.debt_settlement_flag <- levels(as.factor(train_data$debt_settlement_flag))

train_data$debt_settlement_flag_date <- as.numeric(as.factor(train_data$debt_settlement_flag_date)) - 1
train_data.encodings.debt_settlement_flag_date <- levels(as.factor(train_data$debt_settlement_flag_date))

train_data$settlement_status <- as.numeric(as.factor(train_data$settlement_status)) - 1
train_data.encodings.settlement_status <- levels(as.factor(train_data$settlement_status))

train_data$settlement_date <- as.numeric(as.factor(train_data$settlement_date)) - 1
train_data.encodings.settlement_date <- levels(as.factor(train_data$settlement_date))
```

```{r}
test_data$hardship_type <- as.numeric(as.factor(test_data$hardship_type)) - 1
test_data.encodings.hardship_type <- levels(as.factor(test_data$hardship_type))

test_data$hardship_reason <- as.numeric(as.factor(test_data$hardship_reason)) - 1
test_data.encodings.hardship_reason <- levels(as.factor(test_data$hardship_reason))

test_data$hardship_status <- as.numeric(as.factor(test_data$hardship_status)) - 1
test_data.encodings.hardship_status <- levels(as.factor(test_data$hardship_status))

test_data$hardship_start_date <- as.numeric(as.factor(test_data$hardship_start_date)) - 1
test_data.encodings.hardship_start_date <- levels(as.factor(test_data$hardship_start_date))

test_data$hardship_end_date <- as.numeric(as.factor(test_data$hardship_end_date)) - 1
test_data.encodings.hardship_end_date <- levels(as.factor(test_data$hardship_end_date))

test_data$payment_plan_start_date <- as.numeric(as.factor(test_data$payment_plan_start_date)) - 1
test_data.encodings.payment_plan_start_date <- levels(as.factor(test_data$payment_plan_start_date))

test_data$hardship_loan_status <- as.numeric(as.factor(test_data$hardship_loan_status)) - 1
test_data.encodings.hardship_loan_status <- levels(as.factor(test_data$hardship_loan_status))

test_data$debt_settlement_flag <- as.numeric(as.factor(test_data$debt_settlement_flag)) - 1
test_data.encodings.debt_settlement_flag <- levels(as.factor(test_data$debt_settlement_flag))

test_data$debt_settlement_flag_date <- as.numeric(as.factor(test_data$debt_settlement_flag_date)) - 1
test_data.encodings.debt_settlement_flag_date <- levels(as.factor(test_data$debt_settlement_flag_date))

test_data$settlement_status <- as.numeric(as.factor(test_data$settlement_status)) - 1
test_data.encodings.settlement_status <- levels(as.factor(test_data$settlement_status))

test_data$settlement_date <- as.numeric(as.factor(test_data$settlement_date)) - 1
test_data.encodings.settlement_date <- levels(as.factor(test_data$settlement_date))
```

## Cleaning Training Data
### Cleaning Character data
```{r}
training_clean_result <- clean_char_data(train_data)
train_data <- training_clean_result[[1]]
train_data
```
```{r}
# Identify columns with missing values in the training data
training_id_prop_NA_list <- identify_proportion_NA(train_data, verbose=FALSE)
```

```{r}
# Identify the factors in the training data
factors <- train_data %>% select_if(is.factor)
names_factors <- names(factors)
```

```{r}
# Identify the proportion of missing values amongst the factors
### hardship_status
for (i in seq_along(names_factors)){
  describe_proportion_missing(training_id_prop_NA_list, names_factors[i])
  cat("\n")
}
```

```{r}
# Potential categories for sentiment analysis
# emp_title
# title
# url
# desc

# Potential categories for imputation using knn or kmeans
# emp_length
# last_credit_pull_d
# last_pymnt_d

# Only emp_length is in the test data
```

```{r}
# train_data$emp_title
```

```{r}
# Update cleaned data
train_data <- training_id_prop_NA_list[[1]]
```

```{r}
# Dropping emp_title, title, url, & description due to irrelevant content
train_data %<>% select(everything(), -emp_title, -title, -url, -desc)
```

```{r}
### Impute missing values using clustering techniques
# emp_length
# last_credit_pull_d
# last_pymnt_d
```

```{r}
# Dropping emp_length, last_credit_pull_d, & last_pymnt_d because not in test data
train_data %<>% select(everything(), -emp_length, -last_credit_pull_d, -last_pymnt_d)
```

```{r}
# Inverting the good_loan logic to be intuitive
train_data$good_loan <- (as.numeric(train_data$loan_status) - 1) 
```

```{r}
# Dropping redundant loan_status col
train_data %<>% select(everything(), -loan_status)
```

```{r}
train_data
```

## Cleaning the Test Data
### Cleaning Character Data
```{r}
test_clean_result <- clean_char_data(test_data)
```

```{r}
test_data <- test_clean_result[[1]]
```

```{r}
# Identify columns with missing values in the test data
test_id_prop_NA_list <- identify_proportion_NA(test_data, verbose=FALSE)
```

```{r}
# Identify the factors in the test data
factors <- test_data %>% select_if(is.factor)
names_factors <- names(factors)
```

```{r}
# Identify the proportion of missing values amongst the factors
for (i in seq_along(names_factors)){
  describe_proportion_missing(test_id_prop_NA_list, names_factors[i])
  cat("\n")
}
```

```{r}
# Potential to impute values using clustering techniques and data-preprocessing
# emp_title

# Potential to impute values using clustering techniques
# emp_length
```

```{r}
test_data <- test_id_prop_NA_list[[1]]
```

### Impute emp_title using clustering techniques
```{r}
# Imputing emp_length using clustering techniques
# test_data$emp_title
```

```{r}
# Dropping emp_title & emp_length to create classifier; able to compare results after imputation later
test_data %<>% select(everything(), -emp_title, -emp_length, -title, -url, -desc)
```

```{r}
# (test_data)
# (train_data)
```

```{r}
# Test values that were dropped for quantity of missing values or irrelevant content (title, url, desc, emp_title are irrelevant)
dropped_test_values <- c(test_id_prop_NA_list[[2]], test_id_prop_NA_list[[3]], "emp_title", "emp_length", "title", "url", "desc")
dropped_training_values <- c(training_id_prop_NA_list[[2]], training_id_prop_NA_list[[3]], "emp_title", "emp_length", "last_credit_pull_d", "last_pymnt_d", "title", "url", "desc")
```

```{r}
length(dropped_training_values)
length(dropped_test_values)
```

```{r}
# Verifying The length of training is equal to test
dropped_test_values_only <- (dropped_test_values[!(dropped_test_values %in% dropped_training_values)]) # values that were dropped only in test

# dropped_test_values[(dropped_test_values %in% dropped_training_values)] # values that were dropped in both training & test
# dropped_training_values[!(dropped_training_values %in% dropped_test_values)] # unique dropped training values
```


```{r}
values_not_in_train_data <- c("pymnt_plan", "disbursement_method")
dropped_test_values_only <- dropped_test_values_only[!(dropped_test_values_only %in% values_not_in_train_data)]

train_data %<>% select(everything(), -dropped_test_values_only)
```

```{r}
length(names(train_data))
length(names(test_data))
# test_data
```

```{r}
# All data is present & transformed into either numeric or factors
test_data
```

```{r}
train_data
```

```{r}
write.csv(train_data, file = 'clean_train_loans2013_2015q1_minimalprocessing.csv')
```

```{r}
write.csv(test_data, file = 'clean_test_loans2013_2015q1_minimalprocessing.csv')
```

# Data Wrangling to create a balanced training and validation classifier
```{r}
train_data_factors <- train_data %>% select_if(is.factor)
names_train_data_factors <- names(train_data_factors)

test_data_factors <- test_data %>% select_if(is.factor)
names_test_data_factors <- names(test_data_factors)
```

```{r}
# Factor predictors must have at most 32 levels; encoding large factors as numeric in the training data
for (i in seq_along(names_train_data_factors)){
  print(names_train_data_factors[i])
  print(length(levels(train_data[,names_train_data_factors[i]])))
  if(length(levels(train_data[,names_train_data_factors[i]])) > 32){
    train_data[,names_train_data_factors[i]] <- as.numeric(train_data[,names_train_data_factors[i]])
  }
}
```

```{r}
# Factor predictors must have at most 32 levels; encoding large factors as numeric in the test data
for (i in seq_along(names_test_data_factors)){
  print(names_test_data_factors[i])
  print(length(levels(test_data[,names_test_data_factors[i]])))
  if(length(levels(test_data[,names_test_data_factors[i]])) > 32){
    test_data[,names_test_data_factors[i]] <- as.numeric(test_data[,names_test_data_factors[i]])
  }
}

```

```{r}
# Scaling numerically encoded factors between 0 and 1
train_data_max.sub_grade <- which.max(train_data$sub_grade)
train_data_max.zip_code <- which.max(train_data$zip_code)
train_data_max.addr_state <- which.max(train_data$addr_state)
train_data_max.earliest_cr_line <-  which.max(train_data$earliest_cr_line)

train_data$sub_grade <- train_data$sub_grade / train_data_max.sub_grade
train_data$zip_code <- train_data$zip_code / train_data_max.zip_code
train_data$addr_state <- train_data$addr_state / train_data_max.addr_state
train_data$earliest_cr_line <- train_data$earliest_cr_line / train_data_max.earliest_cr_line

# train_data$sub_grade
# train_data$zip_code
# train_data$addr_state
# train_data$earliest_cr_line
```

```{r}
# Scaling test data 
test_data_max.sub_grade <- which.max(test_data$sub_grade)
test_data_max.zip_code <- which.max(test_data$zip_code)
test_data_max.addr_state <- which.max(test_data$addr_state)
test_data_max.earliest_cr_line <-  which.max(test_data$earliest_cr_line)

test_data$sub_grade <- test_data$sub_grade / test_data_max.sub_grade
test_data$zip_code <- test_data$zip_code / test_data_max.zip_code
test_data$addr_state <- test_data$addr_state / test_data_max.addr_state
test_data$earliest_cr_line <- test_data$earliest_cr_line / test_data_max.earliest_cr_line
```

```{r}
# Treating good_loan as a factor to create a classification tree
train_data$good_loan <- as.factor(train_data$good_loan)
```

```{r}
# Rebalancing the data
# 17% of the data are examples of loans that were charged off
nrow(train_data %>% filter(good_loan == 0))/length(train_data$good_loan) * 100
nrow(train_data %>% filter(good_loan == 0))

# Creating Subsets
paid_in_full <- train_data %>% filter(good_loan == 1)
charged_off_subset <- train_data %>% filter(good_loan == 0)

paid_in_full_subset <- sample_n(paid_in_full, nrow(charged_off_subset))

paid_in_full_subset
charged_off_subset
```

```{r}
# Creating a training and validation set
paid_in_full_subset_split_index <- sample(c(1,2), nrow(paid_in_full_subset), replace = TRUE, prob = c(.8, .2))
charged_off_subset_split_index <- sample(c(1,2), nrow(charged_off_subset), replace = TRUE, prob = c(.8, .2))
```

```{r}
paid_in_full_subset_train <- paid_in_full_subset[paid_in_full_subset_split_index==1,]
paid_in_full_subset_validaion <- paid_in_full_subset[paid_in_full_subset_split_index == 2, ]
charged_off_subset_train <- charged_off_subset[charged_off_subset_split_index==1, ]
charged_off_subset_validation <- charged_off_subset[charged_off_subset_split_index==2, ]

train_data_balanced <- add_row(paid_in_full_subset_train, charged_off_subset_train)
validation_data_balanced <- add_row(paid_in_full_subset_validaion, charged_off_subset_validation)
```

```{r}
# Reshuffle the rows
set.seed(42)
rows <- sample(nrow(train_data_balanced))
train_data_balanced <- train_data_balanced[rows, ]
rows <- sample(nrow(validation_data_balanced))
validation_data_balanced <- validation_data_balanced[rows,]
```

```{r}
train_data_balanced
```

```{r}
validation_data_balanced
```

# Exploratory Data Analysis
## Fitting a Lasso Regression
```{r}
# Fitting a lasso regression to see which predictors may be eliminated.
x_train <- train_data_balanced %>% select(everything(), -good_loan)
y_train <- train_data_balanced %>% select(good_loan)

x_test <- validation_data_balanced %>% select(everything(), -good_loan)
y_test <- validation_data_balanced %>% select(good_loan)
```
 
```{r}
x_train_numeric <- x_train %>% mutate_if(is.factor, as.numeric)
y_train_numeric <- y_train %>% mutate_if(is.factor, as.numeric)
x_test_numeric <- x_test %>% mutate_if(is.factor, as.numeric)
y_test_numeric <- y_test %>% mutate_if(is.factor, as.numeric)
```

```{r}
x <- model.matrix(good_loan ~ ., data = train_data_balanced)[, -1] # -1 removes the intercept
y <- train_data_balanced$good_loan
```

```{r}
# grid
typeof(x_train_numeric)
```

```{r}
x_train_numeric_matrix <- as.matrix(x_train_numeric)
```

```{r}
y_train_numeric_matrix <- as.matrix(y_train_numeric)
```

```{r}
x_test_numeric_matrix <- as.matrix(x_test_numeric)
```

```{r}
y_test_numeric_matrix <- as.matrix(y_test_numeric)
```

### How to examine the useful coefficients of a lasso regression:
```{r}
# Create a lasso regression on the training set.
set.seed(42)
grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x_train_numeric_matrix, y_train_numeric_matrix, alpha = 1, lambda = grid)
# plot(lasso.mod)
# summary(lasso.mod)
```

```{r}
# Cross validate the training set to create a lasso regression & identify the minimum value of lambda.
set.seed(42)
cv.out <- cv.glmnet(x_train_numeric_matrix, y_train_numeric_matrix, alpha = 1)
plot(cv.out)
cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s = cv.out$lambda.min, newx = x_test_numeric_matrix)
mean((lasso.pred - y_test_numeric_matrix)^2)
```

```{r}
# Train a lasso regression using the training data
out <- glmnet(x_train_numeric_matrix, y_train_numeric_matrix, lambda = grid, alpha = 1)
```

```{r}
# Combine the training and validation data 
x_all_numeric <- add_row(x_train_numeric, x_test_numeric)
y_all_numeric <- add_row(y_train_numeric, y_test_numeric)

# Convert to a matrix to allow for the creation of a lasso
x_all_numeric_matrix <- as.matrix(x_all_numeric)
y_all_numeric_matrix <- as.matrix(y_all_numeric)
```

```{r}
# Train a lasso regression using the combined datasets
out <- glmnet(x_all_numeric_matrix, y_all_numeric_matrix, lambda = grid, alpha = 1)

# Make a prediction using the minimum lambda from the cross-validated lasso regression & save the coefficients
lasso.coef <- predict(out, s = cv.out$lambda.min, type = "coefficients")[, ]
lasso.coef
```

```{r}
# Identify all the positive coefficients of the lasso regression:
all_coef_names <- names(lasso.coef)
lasso.pos_coef_names <- c(character())
lasso.coef_positive_values <- c(integer())
lasso.index <- 1
verbose = FALSE
for (i in seq_along(lasso.coef)){
  if(lasso.coef[i] > 0) {
    if(verbose){
      print(all_coef_names[i])
      print(lasso.coef[i])      
    }
    lasso.pos_coef_names[lasso.index] <- all_coef_names[i]
    lasso.coef_positive_values[lasso.index] <- lasso.coef[i]
    lasso.index <- lasso.index + 1
  }
}
```

```{r}
lasso.pos_coef_names
```

```{r}
f_print(sprintf('Lasso coefficients that were identified to have a significant effect on the "good loan" response variable include:'))
cat("\n")
f_print(sprintf('%s,', lasso.pos_coef_names[seq(2, (length(lasso.pos_coef_names))-1)]))
cat(" ")
f_print(sprintf(' & %s.',lasso.pos_coef_names[length(lasso.pos_coef_names)]))

```

# Decision Tree Classifier
## Training Decision Tree Classifiers
```{r}
# Removing column id as it has no relevant information with respect to good loan status
train_data_balanced_id <- train_data_balanced$id
validation_data_balanced_id <- validation_data_balanced$id

train_data_balanced %<>% select(everything(), -id)
validation_data_balanced %<>% select(everything(), -id)
```

```{r}
# Examining vif of train_data_balanced
train_data_balanced_integer <- train_data_balanced %>% mutate_if(is.factor, as.numeric)
```

```{r}
train_data_balanced_integer
```

```{r}
# Excluding response "good_loan" as predictor
train_data_balanced_integer_good_loan <- train_data_balanced_integer %>% select(good_loan)
train_data_balanced_integer %<>% select(everything(), -good_loan)
```

```{r}
vif.train_data_balanced <- vif(train_data_balanced_integer)
```

```{r}
vif.train_data_balanced_names <- names(vif.train_data_balanced)
```

```{r}
# Variables that are explained by other variables in the dataset
high_vif.names <- c(character())
high_vif.values <- c(integer())
high_vif.index <- 1
print("Balanced Train Data with VIF scores greater than 5:")
cat("\n")
for (i in seq_along(vif.train_data_balanced_names)){
  if (vif.train_data_balanced[i] > 5){
    print(vif.train_data_balanced[i])
    high_vif.names[high_vif.index] <- vif.train_data_balanced_names[i]
    high_vif.values[high_vif.index] <- vif.train_data_balanced[i]
    high_vif.index <- high_vif.index + 1
    cat("\n")
  }
}
```

```{r}
# Variables that are unique to the data
print("Balanced Train Data with VIF scores less than 5:")
cat("\n")
low_vif.names <- c(character())
low_vif.values <- c(integer())
low_vif.index <- 1
for (i in seq_along(vif.train_data_balanced_names)){
  if (vif.train_data_balanced[i] <= 5){
    print(vif.train_data_balanced[i])
    low_vif.names[low_vif.index] <- vif.train_data_balanced_names[i]
    low_vif.values[low_vif.index] <- vif.train_data_balanced[i]
    low_vif.index <- low_vif.index + 1
    cat("\n")
  }
}
```

```{r}
high_vif.names
high_vif.values
```

```{r}
low_vif.names
low_vif.values
```

```{r}
# Removing columns with VIF scores greater than 5 due to collinearity in the data
train_data_balanced_no_collinearity <- train_data_balanced %>% select(everything(), -high_vif.names)
```

```{r}
train_data_balanced_no_collinearity
```

```{r}
train_data_balanced_no_collinearity
```


```{r}
validation_data_balanced_no_collinearity <- validation_data_balanced %>% select(everything(), -high_vif.names)
```


```{r}
# train_data_balanced_no_collinearity
```

```{r}
validation_data_balanced_no_collinearity
```


# Data Wrangling for Decision Tree Classifier
```{r}
small_sample <- sample(500)
train_data_balanced_no_collinearity_small_sample <- train_data_balanced_no_collinearity[small_sample, ]
```

```{r}
# train_data_balanced_no_collinearity_integer_only <- train_data_balanced_no_collinearity %>% select_if(is.numeric)
```

```{r}
# train_data_balanced_no_collinearity_integer_only <- cbind(train_data_balanced_no_collinearity_integer_only, train_data_balanced_no_collinearity$good_loan)
```

```{r}
# train_data_balanced_no_collinearity_integer_only %<>% rename('good_loan' = `train_data_balanced_no_collinearity$good_loan`)
```

```{r}
# train_data_balanced_no_collinearity_integer_only$good_loan <- as.numeric(train_data_balanced_no_collinearity_integer_only$good_loan)
```

### Fitting the Decision Tree Classifer
```{r}
set.seed(42)
tree.good_loan <- tree(good_loan ~ ., data = train_data_balanced_no_collinearity_small_sample)
summary(tree.good_loan)
```

```{r}
# set.seed(42)
# tree.good_loan <- tree(good_loan ~ ., data = train_data_balanced)
# summary(tree.good_loan)
```

```{r}
plot(tree.good_loan)
text(tree.good_loan, pretty = 0)
```

```{r}
# Recode to factor
# train_data_max.sub_grade* 0.0402098
```

```{r}
tree.pred <- predict(tree.good_loan, validation_data_balanced, type = "class")
accuracy(tree.pred, validation_data_balanced$good_loan)
```

### Pruning the Decision Tree
```{r}
set.seed(42)
cv_tree.good_loan <- cv.tree(tree.good_loan, FUN = prune.misclass)
names(tree.good_loan)
tree.good_loan
```

```{r}
par(mfrow = c(1,2))
plot(cv_tree.good_loan$size, cv_tree.good_loan$dev, type = "b")
plot(cv_tree.good_loan$k, cv_tree.good_loan$dev, type = "b")
```
```{r}
smallest_dev <- cv_tree.good_loan$size[which.min(cv_tree.good_loan$dev)]
```

```{r}
prune.good_loan <- prune.misclass(tree.good_loan, best = smallest_dev)
plot(prune.good_loan)
text(prune.good_loan, pretty = 0)
```

```{r}
tree_pred <- predict(prune.good_loan, validation_data_balanced, type = "class")
accuracy(tree_pred, validation_data_balanced$good_loan)
```

### Creating a Decision Tree using more data
```{r}
set.seed(42)
tree.good_loan <- tree(good_loan ~ ., data = train_data_balanced)
summary(tree.good_loan)
```

```{r}
plot(tree.good_loan)
text(tree.good_loan, pretty = 0)
```

```{r}
tree_pred <- predict(tree.good_loan, validation_data_balanced, type="class")
accuracy(tree_pred, validation_data_balanced$good_loan)
```

```{r}
# length(tree.pred)
validation_data_balanced
```

```{r}
set.seed(42)
cv_tree.good_loan <- cv.tree(tree.good_loan, FUN = prune.misclass) # cv_tree.good_loan
names(tree.good_loan)
tree.good_loan
```

```{r}
par(mfrow = c(1,2))
plot(cv_tree.good_loan$size, cv_tree.good_loan$dev, type = "b")
plot(cv_tree.good_loan$k, cv_tree.good_loan$dev, type = "b")
prune.good_loan <- prune.misclass(tree.good_loan, best = 2)
plot(prune.good_loan)
text(prune.good_loan, pretty = 0)
```

```{r}
tree_pred <- predict(prune.good_loan, validation_data_balanced, type="class")
accuracy(tree_pred, validation_data_balanced$good_loan)
```

## RandomForest
### Bagging and Random Forest
```{r}
# Creating a bagged model using all variables & a small subset of data
set.seed(42)
bag.good_loan_bag <- randomForest(good_loan ~ ., data = train_data_balanced_no_collinearity_small_sample, mtry = 63, importance = TRUE)
yhat.bag <- predict(bag.good_loan_bag, newdata = validation_data_balanced[small_sample, ])

# Plot of ground truth vs. predictions
# X is ground truth, y is prediction
plot(validation_data_balanced[small_sample,]$good_loan, yhat.bag)
```

```{r}
# Accuracy = 1 - MSE
# (1 - mean(((as.numeric(yhat.bag) - 1) - (as.numeric(validation_data_balanced[small_sample, ]$good_loan) - 1))^2)) * 100
```

```{r}
accuracy(yhat.bag, validation_data_balanced[small_sample, ]$good_loan)
```

```{r}
# Creating a bagged model using all variables & a small subset of data and increasing the number of trees.
set.seed(42)
bag.good_loan_random_forest <- randomForest(good_loan ~ ., data = train_data_balanced_no_collinearity_small_sample, mtry = 12, ntree = 25, importance = TRUE)
yhat.bag <- predict(bag.good_loan_random_forest, newdata = validation_data_balanced[small_sample, ])

# Plot of ground truth vs. predictions
# X is ground truth, y is prediction
plot(validation_data_balanced[small_sample,]$good_loan, yhat.bag)

# 1 - MSE
(1 - mean(((as.numeric(yhat.bag) - 1) - (as.numeric(validation_data_balanced[small_sample, ]$good_loan) - 1))^2)) * 100

accuracy(yhat.bag, validation_data_balanced[small_sample, ]$good_loan)
```

```{r}
# Creating a bagged model using all variables & all data
set.seed(42)
bag.good_loan <- randomForest(good_loan ~ ., data = train_data_balanced_no_collinearity, mtry = 63, importance = TRUE)
yhat.bag <- predict(bag.good_loan, newdata = validation_data_balanced)

# Plot of ground truth vs. predictions
# X is ground truth, y is prediction
plot(validation_data_balanced$good_loan, yhat.bag)

# 1 - MSE
(1 - mean(((as.numeric(yhat.bag) - 1) - (as.numeric(validation_data_balanced[small_sample, ]$good_loan) - 1))^2)) * 100
```

### Creating a boosted tree to increase model performance
```{r}
set.seed(42)
boost.good_loan <- gbm(good_loan ~ ., data = train_data_balanced_no_collinearity_small_sample, distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
summary(boost.good_loan)

yhat.boost <- predict(boost.good_loan, newdata = validation_data_balanced[small_sample,], n.trees = 5000)

# MSE of the boosted predictions
boost.mse <- mean(((as.numeric(yhat.boost) - 1) - (as.numeric(validation_data_balanced[small_sample, ]$good_loan) - 1))^2)

```

```{r}
# accuracy of the boosted model
boost.acc <- (1 - boost.mse) * 100
f_print(sprintf("Accuracy of the boosted model: %0.3f%%", boost.acc))
```

```{r}
# Increasing boosted accuracy by reducing the value of lambda
boost.good_loan_with_shrinkage <- gbm(good_loan ~ ., data = train_data_balanced_no_collinearity_small_sample, distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.2)
summary(boost.good_loan_with_shrinkage)
yhat.boost_with_shrinkage <- predict(boost.good_loan_with_shrinkage, newdata = validation_data_balanced[small_sample, ], n.trees = 5000)

# MSE of the boosted predicitons 
boost.mse_with_shrinkage <- as.numeric(yhat.boost_with_shrinkage)
```
### Creating a RandomForest to improve model accuracy
```{r}
# The number of variables in train_data_balanced
length(train_data_balanced)
```

```{r}
set.seed(42)
bag.good_loan <- randomForest(good_loan ~ ., data = train_data_balanced, mtry = 6, importance = TRUE)
```

```{r}
yhat.bag <- predict(bag.good_loan, newdata = validation_data_balanced)
plot((as.numeric(yhat.bag)-1), (as.numeric(validation_data_balanced$good_loan) - 1))
abline(0, 1)
f_print(sprintf("The test MSE of the bagging model is: %0.3f.", mean(((as.numeric(yhat.bag)-1) - (as.numeric(validation_data_balanced$good_loan) - 1)) ^2))) # MSE is .345 @ mtry = 6; .347 otherwise
```

```{r}
importance(bag.good_loan)
```

```{r}
varImpPlot(bag.good_loan)
```


# Logistic Regression on Training & Validation Data with no collinearity 

```{r}
# train_data_balanced_no_collinearity[train_data_balanced_no_collinearity$good_loan == 0, 'good_loan'] <- 2
# train_data_balanced_no_collinearity[train_data_balanced_no_collinearity$good_loan == 1, 'good_loan'] <- 0
# train_data_balanced_no_collinearity[train_data_balanced_no_collinearity$good_loan == 2, 'good_loan'] <- 1
train_data_balanced_no_collinearity$good_loan <-  as.integer(train_data_balanced_no_collinearity$good_loan)
```

```{r}
train_data_balanced_no_collinearity$good_loan <- train_data_balanced_no_collinearity$good_loan - 1
```

```{r}
# train_data_balanced_no_collinearity[train_data_balanced_no_collinearity$good_loan == 0, 'good_loan'] <- 2
# train_data_balanced_no_collinearity[train_data_balanced_no_collinearity$good_loan == 1, 'good_loan'] <- 0
# train_data_balanced_no_collinearity[train_data_balanced_no_collinearity$good_loan == 2, 'good_loan'] <- 1
```

```{r}
train_data_balanced_no_collinearity_corrected_good_loan <- train_data_balanced_no_collinearity
```

```{r}
train_data_balanced_no_collinearity_corrected_good_loan$good_loan <- as.factor(train_data_balanced_no_collinearity$good_loan)
```

```{r}
validation_data_balanced_no_collinearity_good_loan_corrected <- validation_data_balanced_no_collinearity
```

```{r}
validation_data_balanced_no_collinearity_good_loan_corrected$good_loan <- as.integer(validation_data_balanced_no_collinearity_good_loan_corrected$good_loan)
```


```{r}
validation_data_balanced_no_collinearity_good_loan_corrected$good_loan <- validation_data_balanced_no_collinearity_good_loan_corrected$good_loan - 1
```


```{r}
# validation_data_balanced_no_collinearity_good_loan_corrected
```

```{r}
# validation_data_balanced_no_collinearity_good_loan_corrected[validation_data_balanced_no_collinearity_good_loan_corrected$good_loan == 0, 'good_loan'] <- 2
# validation_data_balanced_no_collinearity_good_loan_corrected[validation_data_balanced_no_collinearity_good_loan_corrected$good_loan == 1, 'good_loan'] <- 0
# validation_data_balanced_no_collinearity_good_loan_corrected[validation_data_balanced_no_collinearity_good_loan_corrected$good_loan == 2, 'good_loan'] <- 1
```

```{r}
# validation_data_balanced_no_collinearity_good_loan_corrected has been corrected where 0 is "fully paid" and 1 is "charged off"
validation_data_balanced_no_collinearity_good_loan_corrected$good_loan

```

```{r}
modified_outlier_columns <-  c('total_bal_ex_mort', 'total_acc', 'tot_hi_cred_lim', 'tot_cur_bal', 'tot_coll_amt', 'tax_liens', 'settlement_percentage', 'settlement_amount', 'revol_bal', 'recoveries', 'pub_rec', 'pct_tl_nvr_dlq', 'open_acc', 'num_tl_op_past_12m', 'num_sats', 'num_rev_accts', 'num_op_rev_tl', 'num_il_tl', 'num_bc_tl', 'num_actv_bc_tl', 'mths_since_recent_bc', 'mo_sin_rcnt_tl', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_old_rev_tl_op', 'mo_sin_old_il_acct', 'fico_range_low', 'fico_range_high', 'delinq_amnt', 'delinq_2yrs', 'collection_recovery_fee', 'bc_open_to_buy', 'avg_cur_bal', 'annual_inc')
```

```{r}
important_features <- c('loan_amnt', 'int_rate', 'grade', 'sub_grade', 'annual_inc', 'verfication_status', 'ddi', 'tot_cur_bal', 'total_rev_hi_lim', 'avg_cur_bal')
```


```{r}
train_data_balanced_no_collinearity_corrected_good_loan
```



```{r}
train_data_balanced_no_collinearity_corrected_good_loan_important_features <- train_data_balanced_no_collinearity_corrected_good_loan %>% select('loan_amnt', 'annual_inc', 'verification_status', 'dti', 'good_loan')
# outlier_columns
# target_leakage_columns
```


```{r}
glm.fit <- glm(good_loan ~ ., data = train_data_balanced_no_collinearity_corrected_good_loan, family = "binomial")
glm.prob <- predict(glm.fit, validation_data_balanced_no_collinearity_good_loan_corrected, type = "response")
# glm.pred <- rep(0, length(validation_data_balanced_no_collinearity_good_loan_corrected$good_loan))
# glm.pred[glm.prob > 0.5] <- 1
# table(glm.pred, validation_data_balanced_no_collinearity$good_loan)
```

```{r}
# length(validation_data_balanced_no_collinearity$good_loan)
```


```{r}
nrow(validation_data_balanced_no_collinearity_good_loan_corrected)
```


```{r}
length(glm.prob)
length(glm.pred)
length(validation_data_balanced_no_collinearity_good_loan_corrected$good_loan)
length(train_data_balanced_no_collinearity$good_loan)
```

```{r}
length(glm.prob)
nrow(test_data)
```

```{r}
accuracy(glm.prob, validation_data_balanced_no_collinearity_good_loan_corrected$good_loan)
```



```{r}
# Recoding predicitons such that a 0 indicates a fully paid loan & a 1 indicates a loan that is charged off
# glm.pred[glm.pred == 1] <- 2
# glm.pred[glm.pred == 0] <- 1
# glm.pred[glm.pred == 2] <- 0
# 
# validation_data_balanced_no_collinearity_inverted_results <- as.numeric(validation_data_balanced_no_collinearity$good_loan) - 1
# validation_data_balanced_no_collinearity_inverted_results[validation_data_balanced_no_collinearity_inverted_results == 1] <- 2
# validation_data_balanced_no_collinearity_inverted_results[validation_data_balanced_no_collinearity_inverted_results == 0] <- 1
# validation_data_balanced_no_collinearity_inverted_results[validation_data_balanced_no_collinearity_inverted_results == 2] <- 0

# length(validation_data_balanced_no_collinearity$good_loan)
# length(validation_data_balanced_id)
```

```{r}
# validation_data_balanced_no_collinearity_inverted_results
```

```{r}
results <- data.frame(test_data$id, glm_test_prob)
```

```{r}
results
```



```{r}
# creating an output csv 
results <- data.frame(validation_data_balanced_id, glm.pred)
```

```{r}
results %<>% rename('id' = `test_data.id`)
```

```{r}
results %<>% rename('good_loan' = `glm_test_prob`)
```

```{r}
results
```



```{python}
from sklearn.metrics import log_loss
```

```{r}
glm_pred <- glm.pred
```

```{python}
# r.validation_data_balanced_no_collinearity_inverted_results
r.glm_pred
```

```{python}
log_loss_val = log_loss(r.validation_data_balanced_no_collinearity_good_loan_corrected.good_loan, r.glm_pred)
```

```{python}
log_loss_val
```

```{python}
log_loss_val
```

```{r}
test_data
```

```{r}
glm.prob <- predict(glm.fit, test_data, type = "response")
glm.pred <- rep(0, nrow(test_data))
glm.pred[glm.prob > 0.5] <- 1
# glm.pred[glm.pred == 1] <- 2
# glm.pred[glm.pred == 0] <- 1
# glm.pred[glm.pred == 2] <- 0
```

```{r}
for (i in seq(glm.pred)){
  glm.pred[i] <- 0
}
```

```{r}
glm.pred
```

```{r}
# glm.pred
# test_data$id
results <- data.frame(test_data$id, glm.pred)
results %<>% rename('id' = `test_data.id`)
results %<>% rename('good_loan' = `glm.pred`)
```

```{r}
results
```


```{r}
write.csv(results, 'submission_4.csv', row.names = FALSE)
```



```{r}
sample_submission <- read.csv('./sample_submission.csv')
```

# Reading Rashmi Bantha's Cleaned data for comparison
```{r}
rb_cleaned_train <- read_csv('./train_cleaned_jan24.csv')
rb_cleaned_test <- read_csv('./test_cleaned_jan24.csv')
```

```{r}
train_data_balanced_no_collinearity
```

```{r}
# rb_cleaned_train$good_loan
length(rb_cleaned_test$good_loan)
```

```{r}
train_index <- sample(c(1,2), length(rb_cleaned_train$good_loan), replace = TRUE, prob = c(0.8, 0.2))
```

```{r}
rb_cleaned_train_set <- rb_cleaned_train[train_index == 1, ]
rb_cleaned_validation_set <- rb_cleaned_train[train_index == 2,]
```

```{r}
# Rashmi Bantha's Cleaned data used to create a logistic regression for comparison of performance
glm.fit <- glm(good_loan ~ ., data = rb_cleaned_train_set, family = "binomial")
glm.prob <- predict(glm.fit, rb_cleaned_validation_set, type = "response")
glm.pred <- rep(0, length(rb_cleaned_validation_set$good_loan))
glm.pred[glm.prob > 0.5] <- 1
```

```{r}
accuracy(glm.pred, rb_cleaned_validation_set$good_loan)
```

```{r}
rb_glm_pred <- glm.pred
```

```{python}
log_loss(r.rb_cleaned_validation_set.good_loan, r.rb_glm_pred)
```

### The data needs to be cleaned to improve model performance
- use datarobot to remove features with target leakage
- remove outliers from the data


```{r}
# Target Leakage According to Datarobot
# loan_status
# last_fico_range_high
# last_fico_range_low

# Outliers & inliers 
# pct_tl_nvr_dlq

# Outliers According to Datarobot
# total_rev_hi_lim
# total_rec_late_fee
# total_rec_int
# total_il_high_credit_limit
# total_bc_limit
# total_bal_ex_mort
# total_acc
# tot_hi_cred_lim
# tot_cur_bal
# tot_coll_amt
# tax_liens
# settlement_percentage
# settlement_amount
# revol_bal
# recoveries
# pub_rec
# pct_tl_nvr_dlq
# open_acc
# num_tl_op_past_12m
# num_sats
# num_rev_accts
# num_op_rev_tl
# num_il_tl
# num_bc_tl
# num_actv_bc_tl
# mths_since_recent_bc
# mo_sin_rcnt_tl
# mo_sin_rcnt_rev_tl_op
# mo_sin_old_rev_tl_op
# mo_sin_old_il_acct
# fico_range_low
# fico_range_high
# delinq_amnt
# delinq_2yrs
# collection_recovery_fee
# bc_open_to_buy
# avg_cur_bal
# annual_inc

target_leakage_columns <- c('loan_status', 'last_fico_range_high', 'last_fico_range_low')

modified_outlier_columns <-  c('total_bal_ex_mort', 'total_acc', 'tot_hi_cred_lim', 'tot_cur_bal', 'tot_coll_amt', 'tax_liens', 'settlement_percentage', 'settlement_amount', 'revol_bal', 'recoveries', 'pub_rec', 'pct_tl_nvr_dlq', 'open_acc', 'num_tl_op_past_12m', 'num_sats', 'num_rev_accts', 'num_op_rev_tl', 'num_il_tl', 'num_bc_tl', 'num_actv_bc_tl', 'mths_since_recent_bc', 'mo_sin_rcnt_tl', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_old_rev_tl_op', 'mo_sin_old_il_acct', 'fico_range_low', 'fico_range_high', 'delinq_amnt', 'delinq_2yrs', 'collection_recovery_fee', 'bc_open_to_buy', 'avg_cur_bal', 'annual_inc')

# `total_rev_hi_lim`, `total_rec_late_fee`, `total_rec_int`, `total_il_high_credit_limit`, `total_bc_limit`, etc. don't exist.


outlier_columns <- c('total_rev_hi_lim', 'total_rec_late_fee', 'total_rec_int', 'total_il_high_credit_limit', 'total_bc_limit', 'total_bal_ex_mort', 'total_acc', 'tot_hi_cred_lim', 'tot_cur_bal', 'tot_coll_amt', 'tax_liens', 'settlement_percentage', 'settlement_amount', 'revol_bal', 'recoveries', 'pub_rec', 'pct_tl_nvr_dlq', 'open_acc', 'num_tl_op_past_12m', 'num_sats', 'num_rev_accts', 'num_op_rev_tl', 'num_il_tl', 'num_bc_tl', 'num_actv_bc_tl', 'mths_since_recent_bc', 'mo_sin_rcnt_tl', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_old_rev_tl_op', 'mo_sin_old_il_acct', 'fico_range_low', 'fico_range_high', 'delinq_amnt', 'delinq_2yrs', 'collection_recovery_fee', 'bc_open_to_buy', 'avg_cur_bal', 'annual_inc')
```


```{r}
# Original Data for comparison
original_train_data <- read.csv('./train_loans2013_2015q1_minimalprocessing.csv')
original_test_data <- read.csv('./test_loans2013_2015q1_minimalprocessing.csv')
```


```{r}
train_data_balanced_no_collinearity$last_fico_range_low
# original_train_data$last_fico_range_high
```

```{r}
outlier_columns
```


```{r}
outlier_columns[outlier_columns %in% names(rb_cleaned_train)]
```
```{r}
plot(rb_cleaned_train$avg_cur_bal)
```

```{r}
# Identifying outliers and high-leverage values to increase model performance. 
lm.fit <- lm(good_loan ~ ., data = rb_cleaned_train)
```

```{r}
# Identify interactions with high p-values to increase model performance. 
```

```{r}

```




